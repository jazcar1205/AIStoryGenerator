{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jazcar1205/AIStoryGenerator/blob/main/MADEMiniProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOKk65lmM6fI"
      },
      "source": [
        "Mini Project: Implement ARM or LVM\n",
        "\n",
        "Model chosen: MADE ( Masked Autoencoder for Distribution Estimation.\n",
        "\n",
        "Topic: Satellite Image Analysis for Damage Assessment: Generate pre- and post-disaster satellite images to assess damages automatically\n",
        "\n",
        "Dataset: xView2 Challenge Training set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osZe_qbDfmxh",
        "outputId": "5f9f9b99-16af-4bcc-e5b7-38274f4a1d6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "IMAGES_PATH = '/content/drive/MyDrive/TrainingSet/images'\n",
        "\n",
        "SPLIT = ''\n",
        "\n",
        "\n",
        "NUM_COMPONENTS = 5\n",
        "OUTPUT_PIXEL_DIM = 3 * NUM_COMPONENTS\n",
        "NUM_HIDDEN_LAYERS = 4\n",
        "HIDDEN_SIZE = 1024\n",
        "NUM_WORKERS = 0\n",
        "PATCH_SIZE = 32\n",
        "IMAGE_BANDS = 3\n",
        "INPUT_DIM = PATCH_SIZE * PATCH_SIZE * IMAGE_BANDS * 2\n",
        "NUM_COMPONENTS = 5  # K=5 Gaussian components\n",
        "OUTPUT_PIXEL_DIM = 3 * NUM_COMPONENTS  # (mean, log_std, mixing_weight) * K\n",
        "MADE_OUTPUT_DIM = INPUT_DIM * OUTPUT_PIXEL_DIM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "IMAGES_PATH = '/content/drive/MyDrive/TrainingSet/images'\n",
        "\n",
        "print(\"--- First 5 image files in your directory ---\")\n",
        "try:\n",
        "    # Use os.listdir to get all file names in the directory\n",
        "    all_files = os.listdir(IMAGES_PATH)\n",
        "\n",
        "    # Filter to only get the first 5 entries\n",
        "    for i, filename in enumerate(all_files):\n",
        "        if i < 5:\n",
        "            # Print the actual file name found on disk\n",
        "            print(filename)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if not all_files:\n",
        "        print(f\"Error: The directory {IMAGES_PATH} appears to be empty.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The directory {IMAGES_PATH} was not found. Check the path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daDImn_Ptv3O",
        "outputId": "9348a598-233e-4c09-a599-23da698bdf12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- First 5 image files in your directory ---\n",
            "socal-fire_00000552_post_disaster.png\n",
            "socal-fire_00000593_pre_disaster.png\n",
            "socal-fire_00000553_post_disaster.png\n",
            "socal-fire_00000576_pre_disaster.png\n",
            "socal-fire_00000585_pre_disaster.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class XView2RawDataIndexer (Dataset):\n",
        "    \"\"\"\n",
        "    Indexer: Finds paired pre/post image paths and calculates a fixed center crop.\n",
        "    Damage labels and JSON files are entirely ignored.\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, label_dir=None, patch_size=32):\n",
        "        # NOTE: label_dir is now optional but kept for compatibility.\n",
        "        self.patch_size = patch_size\n",
        "        self.samples = []\n",
        "        self.image_dir = image_dir\n",
        "        self.half_p = self.patch_size // 2\n",
        "\n",
        "        # 1. Find all PRE-DISASTER image files (not JSONs)\n",
        "        # Use recursive search to handle possible subfolders\n",
        "        image_files = glob.glob(os.path.join(self.image_dir, '**', '*_pre_disaster.png'), recursive=True)\n",
        "\n",
        "        for pre_image_path in tqdm(image_files, desc=\"Indexing Image Patches\"):\n",
        "\n",
        "            # Construct the corresponding POST-DISASTER image path\n",
        "            post_filename = os.path.basename(pre_image_path).replace('_pre_disaster.png', '_post_disaster.png')\n",
        "\n",
        "            # ASSUMING: post image is in the same directory as pre image\n",
        "            post_image_path = os.path.join(os.path.dirname(pre_image_path), post_filename)\n",
        "\n",
        "            # Skip if image pair is missing\n",
        "            if not os.path.exists(post_image_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Load PRE image just to get its size (needed for center crop)\n",
        "                temp_img = Image.open(pre_image_path)\n",
        "                img_width, img_height = temp_img.size\n",
        "                temp_img.close()\n",
        "            except Exception:\n",
        "                continue # Skip corrupted images\n",
        "\n",
        "            # --- Simplified Crop Box Calculation (Center of Image) ---\n",
        "            # NOTE: This replaces the building-centric polygon logic entirely.\n",
        "\n",
        "            # Calculate the center of the full image\n",
        "            center_x, center_y = img_width // 2, img_height // 2\n",
        "\n",
        "            # Calculate crop box centered on the image\n",
        "            crop_left = max(0, center_x - self.half_p)\n",
        "            crop_top = max(0, center_y - self.half_p)\n",
        "            crop_right = min(img_width, center_x + self.half_p)\n",
        "            crop_bottom = min(img_height, center_y + self.half_p)\n",
        "\n",
        "            # Final check: skip if patch size is zero or too small\n",
        "            if (crop_right - crop_left) != self.patch_size or \\\n",
        "               (crop_bottom - crop_top) != self.patch_size:\n",
        "                # This should only happen if the source image is too small\n",
        "                continue\n",
        "\n",
        "            # Append sample with fixed center crop box\n",
        "            self.samples.append({\n",
        "                'pre_path': pre_image_path,\n",
        "                'post_path': post_image_path,\n",
        "                'crop_box': (crop_left, crop_top, crop_right, crop_bottom),\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Implementation is in XView2MADETransformDataset, which inherits this\n",
        "        raise NotImplementedError(\"Use the XView2MADETransformDataset subclass.\")\n",
        "\n",
        "\n",
        "class XView2MADETransformDataset(XView2RawDataIndexer):\n",
        "    \"\"\"\n",
        "    Transforms image patches into a single, flattened MADE input vector.\n",
        "    Vector structure: [Flatten(Pre-Patch), Flatten(Post-Patch)]\n",
        "    \"\"\"\n",
        "    def __init__(self, image_dir, label_dir=None, transform=None, patch_size=32):\n",
        "        # Note: label_dir is now ignored in the super() call\n",
        "        super().__init__(image_dir, label_dir, patch_size)\n",
        "        # Use simple ToTensor() transformation\n",
        "        self.transform = transform if transform else transforms.ToTensor()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "        crop_left, crop_top, crop_right, crop_bottom = sample['crop_box']\n",
        "\n",
        "        # Load and Crop Images\n",
        "        pre_img = Image.open(sample['pre_path']).convert('RGB')\n",
        "        post_img = Image.open(sample['post_path']).convert('RGB')\n",
        "\n",
        "        pre_patch = pre_img.crop((crop_left, crop_top, crop_right, crop_bottom))\n",
        "        post_patch = post_img.crop((crop_left, crop_top, crop_right, crop_bottom))\n",
        "\n",
        "        # Resize patch if needed (shouldn't be needed with center crop, but keep as safety)\n",
        "        if pre_patch.size != (self.patch_size, self.patch_size):\n",
        "            pre_patch = pre_patch.resize((self.patch_size, self.patch_size))\n",
        "            post_patch = post_patch.resize((self.patch_size, self.patch_size))\n",
        "\n",
        "        # Convert to Tensor (scales to [0, 1]) and Flatten\n",
        "        pre_patch_tensor = self.transform(pre_patch).float()\n",
        "        post_patch_tensor = self.transform(post_patch).float()\n",
        "\n",
        "        # FINAL MADE INPUT VECTOR: [Flatten(Pre-Patch), Flatten(Post-Patch)]\n",
        "        x_made = torch.cat([pre_patch_tensor.flatten(), post_patch_tensor.flatten()])\n",
        "\n",
        "        return x_made"
      ],
      "metadata": {
        "id": "PjE-ZlxiPkCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. NEW CONSTANT ---\n",
        "NUM_COMPONENTS = 5  # Number of Gaussian components (K=5 is common)\n",
        "\n",
        "# The output dimension must be K * 3 for each pixel:\n",
        "# (INPUT_DIM/2) pixels * 3 parameters * K components\n",
        "OUTPUT_PIXEL_DIM = 3 * NUM_COMPONENTS\n",
        "TOTAL_OUTPUT_DIM = (INPUT_DIM // 2) * OUTPUT_PIXEL_DIM\n",
        "# The full MADE output is P(X_pre, X_post), so it must predict the full vector,\n",
        "# but only the X_post pixels use MoG parameters.\n",
        "# We'll stick to INPUT_DIM * 3 * K for simplicity and mask the loss.\n",
        "MADE_OUTPUT_DIM = INPUT_DIM * OUTPUT_PIXEL_DIM\n",
        "\n",
        "\n",
        "# --- 2. MOLECULAR SAMPLING FUNCTION (Replaces the old made_sample_post_disaster) ---\n",
        "def made_sample_mog(model, pre_patch_tensor, patch_size, image_bands, device):\n",
        "    \"\"\"\n",
        "    Generates X_post by sequential sampling from the MoG distribution\n",
        "    predicted by MADE, conditioned on X_pre.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    pre_flat = pre_patch_tensor.flatten().to(device)\n",
        "    post_flat_len = patch_size * patch_size * image_bands\n",
        "\n",
        "    # Initialize the full MADE vector: [X_pre (known), X_post (unknown/zeros)]\n",
        "    x_made = torch.cat([pre_flat, torch.zeros(post_flat_len, device=device)])\n",
        "\n",
        "    start_index = post_flat_len\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(start_index, start_index + post_flat_len):\n",
        "            # Output contains the MoG parameters for all D dimensions\n",
        "            output = model(x_made.unsqueeze(0))\n",
        "\n",
        "            # --- Extract MoG parameters for the current pixel i ---\n",
        "            # The output vector is [D_1_params, D_2_params, ..., D_D_params]\n",
        "            # Each D_i_params is (3 * K) long\n",
        "            pixel_index_in_output = i - (INPUT_DIM // 2)\n",
        "\n",
        "            # Index into the second half of the output vector (P_post)\n",
        "            param_start = pixel_index_in_output * OUTPUT_PIXEL_DIM\n",
        "            param_end = param_start + OUTPUT_PIXEL_DIM\n",
        "            params = output[0, param_start:param_end]\n",
        "\n",
        "            # --- Decompose and Sample ---\n",
        "\n",
        "            # 1. Mixing Weights (pi) - logits -> probabilities\n",
        "            pi_logits = params[0*NUM_COMPONENTS : 1*NUM_COMPONENTS]\n",
        "            pi_probs = F.softmax(pi_logits, dim=0)\n",
        "\n",
        "            # 2. Means (mu)\n",
        "            mu = params[1*NUM_COMPONENTS : 2*NUM_COMPONENTS]\n",
        "\n",
        "            # 3. Log-Scales (log_sigma) -> standard deviations\n",
        "            log_sigma = params[2*NUM_COMPONENTS : 3*NUM_COMPONENTS]\n",
        "            sigma = torch.exp(log_sigma)\n",
        "\n",
        "            # Select a component based on pi\n",
        "            component_index = torch.multinomial(pi_probs, 1).item()\n",
        "\n",
        "            # Sample from the selected Gaussian\n",
        "            sampled_mean = mu[component_index]\n",
        "            sampled_std = sigma[component_index]\n",
        "\n",
        "            # Add small noise to enforce sampling (standard N(mu, sigma) sampling)\n",
        "            sampled_value = sampled_mean + sampled_std * torch.randn(1, device=device)\n",
        "\n",
        "            # Set the sampled value back into the input vector for the next step.\n",
        "            x_made[i] = sampled_value.clamp(0, 1) # Clamp to [0, 1] range\n",
        "\n",
        "    # Extract and Reshape the Generated X_post\n",
        "    generated_post_flat = x_made[start_index:]\n",
        "    generated_post_patch = generated_post_flat.reshape(image_bands, patch_size, patch_size)\n",
        "\n",
        "    return generated_post_patch.cpu()\n",
        "\n",
        "\n",
        "# --- 3. MOLECULAR LOSS FUNCTION (Replaces the old MSE Loss) ---\n",
        "# --- GLOBAL CONSTANTS (REQUIRED FOR LOSS FUNCTION) ---\n",
        "\n",
        "def log_prob_mog(x, pi_logits, mu, log_sigma):\n",
        "    \"\"\"Calculates the log-probability of data x under the MoG distribution.\"\"\"\n",
        "\n",
        "    # Expand x to match mu/sigma dimensions (Batch, D, K)\n",
        "    x = x.unsqueeze(-1).expand_as(mu)\n",
        "\n",
        "    # Calculate log(N(x | mu, sigma))\n",
        "    log_probs = -0.5 * torch.log(2 * torch.pi * torch.ones_like(log_sigma))\n",
        "    log_probs -= log_sigma\n",
        "    log_probs -= 0.5 * ((x - mu) / torch.exp(log_sigma))**2\n",
        "\n",
        "    # Calculate log P(x) = log( sum_k (exp(log(pi_k) + log P(x | component=k))) )\n",
        "    log_pi = F.log_softmax(pi_logits, dim=-1)\n",
        "    log_conditional = log_pi + log_probs\n",
        "\n",
        "    # LogSumExp trick\n",
        "    log_p_x = torch.logsumexp(log_conditional, dim=-1)\n",
        "\n",
        "    # Negative Log-Likelihood (NLL) Loss: -log P(x)\n",
        "    return -log_p_x\n",
        "\n",
        "\n",
        "def MoG_loss(output, target):\n",
        "    \"\"\"\n",
        "    Calculates the Negative Log-Likelihood (NLL) loss for the MADE model.\n",
        "    Only computes loss on the X_post half of the vector (P(X_post | X_pre)).\n",
        "    \"\"\"\n",
        "    batch_size = target.size(0)\n",
        "    data_dim = target.size(1)\n",
        "\n",
        "    # Reshape the MADE output: (Batch, D * (3*K)) -> (Batch, D, 3, K)\n",
        "    # The output dimension is now 3*K times the input dimension.\n",
        "    output = output.reshape(batch_size, data_dim, 3, NUM_COMPONENTS)\n",
        "\n",
        "    # Decompose parameters: (Batch, D, K)\n",
        "    pi_logits = output[:, :, 0, :]\n",
        "    mu = output[:, :, 1, :]\n",
        "    log_sigma = output[:, :, 2, :]\n",
        "\n",
        "    # Clamp log_sigma to prevent numerical instability during exp()\n",
        "    log_sigma = torch.clamp(log_sigma, min=-7.0, max=3.0)\n",
        "\n",
        "    # Calculate the log-probability for every dimension (pixel) in the batch\n",
        "    log_probs_per_dim = log_prob_mog(target, pi_logits, mu, log_sigma)\n",
        "\n",
        "    # We only care about the X_post half of the vector (P(X_post | X_pre))\n",
        "    post_half_start = data_dim // 2\n",
        "\n",
        "    # Loss is the mean of the sum of NLL over all X_post dimensions\n",
        "    # .sum(dim=1) sums over all pixels in X_post. .mean() averages over the batch.\n",
        "    loss = log_probs_per_dim[:, post_half_start:].sum(dim=1).mean()\n",
        "\n",
        "    return loss, loss.item() # Return tensor and scalar loss"
      ],
      "metadata": {
        "id": "ve6Q-5G6Ppwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLinear(nn.Linear):\n",
        "    \"\"\"\n",
        "    A Linear layer with an applied mask to enforce the autoregressive property.\n",
        "    This class remains unchanged from your original implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features, out_features, bias=True):\n",
        "        super().__init__(in_features, out_features, bias)\n",
        "        self.register_buffer('mask', torch.ones(out_features, in_features))\n",
        "\n",
        "    def set_mask(self, mask):\n",
        "        self.mask.copy_(mask)\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.linear(input, self.weight * self.mask, self.bias)\n",
        "\n",
        "\n",
        "class MADE(nn.Module):\n",
        "    \"\"\"\n",
        "    Masked Autoencoder for Distribution Estimation, upgraded to output\n",
        "    Mixture of Gaussian (MoG) parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=INPUT_DIM, hidden_size=HIDDEN_SIZE,\n",
        "                 num_hidden_layers=NUM_HIDDEN_LAYERS, activation='relu'):\n",
        "        super().__init__()\n",
        "\n",
        "        # Output is now the global MoG dimension\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = MADE_OUTPUT_DIM  # <-- CRITICAL CHANGE\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "\n",
        "        activation_map = {'relu': nn.ReLU(), 'tanh': nn.Tanh()}\n",
        "        self.activation = activation_map[activation]\n",
        "\n",
        "        layers = []\n",
        "        # Input layer\n",
        "        layers.append(MaskedLinear(self.input_dim, self.hidden_size))\n",
        "        layers.append(self.activation)\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(self.num_hidden_layers - 1):\n",
        "            layers.append(MaskedLinear(self.hidden_size, self.hidden_size))\n",
        "            layers.append(self.activation)\n",
        "\n",
        "        # Output layer (now targets the MoG dimension)\n",
        "        layers.append(MaskedLinear(self.hidden_size, self.output_dim))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "        self.m = {}\n",
        "        self.d = {}\n",
        "        self.initialize_masks()\n",
        "\n",
        "    def initialize_masks(self):\n",
        "        \"\"\"Initializes the masks (M) and dependency vectors (m) for all layers.\"\"\"\n",
        "        L = self.num_hidden_layers + 1\n",
        "\n",
        "        # 1. Define input/output connectivity vector (m_k)\n",
        "        self.m[0] = torch.arange(self.input_dim, device=device) # Input connectivity\n",
        "\n",
        "        # Hidden Layer connectivity (k=1 to L-1) - UNCHANGED\n",
        "        for l in range(1, L):\n",
        "            min_prev = torch.min(self.m[l-1]).item()\n",
        "            max_prev = torch.max(self.m[l-1]).item()\n",
        "            self.m[l] = torch.randint(min_prev, max_prev + 1, (self.hidden_size,), device=device)\n",
        "\n",
        "        # Output layer connectivity (m_L): CRITICAL CHANGE\n",
        "        # Each output unit (3*K parameters) must depend only on input units with a SMALLER index.\n",
        "        # Original pixel indices: [0, 1, 2, ..., 6143]\n",
        "        pixel_indices = torch.arange(self.input_dim, device=device)\n",
        "        # Repeat each pixel index (3*K) times: [0,0,0..., 1,1,1..., 6143,6143,...]\n",
        "        # This ensures all parameters for pixel i depend on i-1 or less.\n",
        "        self.m[L] = pixel_indices.repeat_interleave(OUTPUT_PIXEL_DIM)\n",
        "\n",
        "\n",
        "        # 2. Compute and set masks (M_k) - Logic remains the same, using new m[L]\n",
        "        layers = [l for l in self.net if isinstance(l, MaskedLinear)]\n",
        "\n",
        "        for l in range(L):\n",
        "            mask = torch.zeros(self.m[l+1].shape[0], self.m[l].shape[0], device=device)\n",
        "\n",
        "            # Input and Hidden Layers (l < L-1)\n",
        "            if l < L - 1:\n",
        "                # M_l(i, j) = 1 if m_l(i) >= m_{l-1}(j)\n",
        "                for i in range(self.m[l+1].shape[0]):\n",
        "                    for j in range(self.m[l].shape[0]):\n",
        "                        if self.m[l+1][i] >= self.m[l][j]:\n",
        "                            mask[i, j] = 1\n",
        "            # Output Layer (l = L-1)\n",
        "            else:\n",
        "                # M_L(i, j) = 1 if m_L(i) > m_{L-1}(j) (Strict inequality)\n",
        "                for i in range(self.m[l+1].shape[0]):\n",
        "                    for j in range(self.m[l].shape[0]):\n",
        "                        if self.m[l+1][i] > self.m[l][j]:\n",
        "                            mask[i, j] = 1\n",
        "\n",
        "            # Set the mask on the corresponding linear layer\n",
        "            layers[l].set_mask(mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "dU5IJrYFuwpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the dataset\n",
        "print(\"Starting data indexing...\")\n",
        "# --- CHANGE: Removed label_dir argument ---\n",
        "full_dataset = XView2MADETransformDataset(\n",
        "    image_dir=IMAGES_PATH,\n",
        "    # label_dir=LABELS_PATH, # <-- REMOVED\n",
        "    patch_size=PATCH_SIZE\n",
        ")\n",
        "# -------------------------------------------\n",
        "\n",
        "total_samples = len(full_dataset)\n",
        "print(f\"\\nâœ… Total building samples loaded for training: {total_samples}\")\n",
        "\n",
        "if total_samples > 0:\n",
        "    # Define split sizes\n",
        "    # NOTE: Ensure 'random_split' is imported from torch.utils.data\n",
        "    train_size = int(0.8 * total_samples)\n",
        "    val_size = total_samples - train_size\n",
        "\n",
        "    # Split the dataset\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    print(f\"Training Samples: {len(train_dataset)}\")\n",
        "    print(f\"Validation Samples: {len(val_dataset)}\")\n",
        "\n",
        "    # Create DataLoaders\n",
        "    BATCH_SIZE = 64\n",
        "    NUM_WORKERS = 0 # Set to 0 for Colab/Notebook environments to avoid multiprocessing issues\n",
        "\n",
        "    # NOTE: Ensure 'DataLoader' is imported from torch.utils.data\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    print(\"\\nDataLoaders created successfully.\")\n",
        "else:\n",
        "    print(\"\\nðŸ›‘ Cannot proceed: No samples were loaded. Double-check image paths and the file search logic.\")\n",
        "\n",
        "\n",
        "try:\n",
        "    made_model = MADE(\n",
        "        input_dim=INPUT_DIM,\n",
        "        hidden_size=HIDDEN_SIZE,\n",
        "        num_hidden_layers=NUM_HIDDEN_LAYERS\n",
        "    ).to(device)\n",
        "\n",
        "    print(\"\\n--- MADE MoG Model Summary ---\")\n",
        "    print(f\"Model instantiated with {NUM_HIDDEN_LAYERS} hidden layers.\")\n",
        "    print(f\"Total parameters: {sum(p.numel() for p in made_model.parameters()):,}\")\n",
        "    print(f\"Final Output Layer Size (MoG): {made_model.output_dim}\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"\\nðŸ›‘ ERROR: The 'MADE' class or 'MaskedLinear' class is not defined. Please ensure you ran the cell defining the MADE MoG architecture.\")"
      ],
      "metadata": {
        "id": "AfJvGKjSu3EC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1544d37-a3e3-447e-963f-d2960fb221d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting data indexing...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing Image Patches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2799/2799 [01:22<00:00, 34.01it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… Total building samples loaded for training: 2799\n",
            "Training Samples: 2239\n",
            "Validation Samples: 560\n",
            "\n",
            "DataLoaders created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Training Function Structure (UPDATED) ---\n",
        "def train_made(model, loader, optimizer, num_epochs):\n",
        "    \"\"\"\n",
        "    Trains the MADE MoG model using Negative Log-Likelihood loss.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        running_loss = 0.0\n",
        "\n",
        "        if loader is None:\n",
        "            print(\"ðŸ›‘ WARNING: Data Loader is None. Skipping training loop definition.\")\n",
        "            break\n",
        "\n",
        "        for batch_idx, data in enumerate(tqdm(loader, desc=f\"Epoch {epoch}/{num_epochs}\")):\n",
        "\n",
        "            data = data.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(data)\n",
        "\n",
        "            # Calculate loss (NOW USING MoG_loss)\n",
        "            # MoG_loss returns: (total_loss_tensor, total_loss_scalar)\n",
        "            total_loss, avg_loss_scalar = MoG_loss(output, data) # <-- FIXED LINE\n",
        "\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += avg_loss_scalar\n",
        "\n",
        "        avg_loss = running_loss / len(loader)\n",
        "        print(f\"Epoch {epoch} Complete. Avg Total Loss (NLL): {avg_loss:.4f}\")\n",
        "\n",
        "train_made(made_model, train_loader, optimizer, NUM_EPOCHS)"
      ],
      "metadata": {
        "id": "22um_v0yvBXA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0304660-0091-4480-ab38-695e2bb5c982"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [09:44<00:00, 16.69s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Complete. Avg Total Loss: 0.1056\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [04:04<00:00,  6.99s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Complete. Avg Total Loss: 0.0557\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:52<00:00,  6.66s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Complete. Avg Total Loss: 0.0339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:54<00:00,  6.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 Complete. Avg Total Loss: 0.0251\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:58<00:00,  6.80s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 Complete. Avg Total Loss: 0.0211\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:49<00:00,  6.55s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 Complete. Avg Total Loss: 0.0179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:57<00:00,  6.79s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 Complete. Avg Total Loss: 0.0158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:50<00:00,  6.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 Complete. Avg Total Loss: 0.0143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:46<00:00,  6.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 Complete. Avg Total Loss: 0.0134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [04:04<00:00,  6.97s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 Complete. Avg Total Loss: 0.0125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:50<00:00,  6.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 Complete. Avg Total Loss: 0.0119\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:48<00:00,  6.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 Complete. Avg Total Loss: 0.0114\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [04:08<00:00,  7.11s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 Complete. Avg Total Loss: 0.0111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:46<00:00,  6.48s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 Complete. Avg Total Loss: 0.0107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:56<00:00,  6.75s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 Complete. Avg Total Loss: 0.0104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:48<00:00,  6.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 Complete. Avg Total Loss: 0.0101\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:57<00:00,  6.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 Complete. Avg Total Loss: 0.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:58<00:00,  6.83s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 Complete. Avg Total Loss: 0.0096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:48<00:00,  6.52s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 Complete. Avg Total Loss: 0.0094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35/35 [03:57<00:00,  6.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 Complete. Avg Total Loss: 0.0092\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "def made_sample_post_disaster(model, pre_patch_tensor, patch_size, image_bands, device):\n",
        "    \"\"\"\n",
        "    Generates the post-disaster patch (X_post) by sampling from the MADE model\n",
        "    conditioned on the pre-disaster patch (X_pre).\n",
        "\n",
        "    Since MADE is an autoregressive model P(x_1, ..., x_D), we feed the first\n",
        "    half (X_pre) and sequentially sample the second half (X_post).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # 1. Prepare the input vector X: [X_pre, X_post(zeros)]\n",
        "    # X_pre is the input conditioning. It must be flattened.\n",
        "    pre_flat = pre_patch_tensor.flatten().to(device)\n",
        "\n",
        "    # X_post is initially a vector of zeros/unknowns.\n",
        "    post_flat_len = patch_size * patch_size * image_bands\n",
        "\n",
        "    # Initialize the full MADE vector: [X_pre (known), X_post (unknown/zeros)]\n",
        "    # We use a copy of the pre_flat to start.\n",
        "    x_made = torch.cat([pre_flat, torch.zeros(post_flat_len, device=device)])\n",
        "\n",
        "    # 2. Sequential Sampling Loop\n",
        "    # The output dimension (D) is the full vector length (INPUT_DIM).\n",
        "    # We only need to sample indices corresponding to X_post (the second half).\n",
        "\n",
        "    start_index = post_flat_len # Start index of X_post pixels\n",
        "\n",
        "    # Iterate through every pixel dimension in X_post\n",
        "    with torch.no_grad():\n",
        "        for i in range(start_index, start_index + post_flat_len):\n",
        "            # The current partial vector is used to predict the next pixel value.\n",
        "            # We predict the entire vector, but only use the i-th prediction.\n",
        "\n",
        "            # MADE outputs logits/means for each output dimension\n",
        "            output = model(x_made.unsqueeze(0))\n",
        "\n",
        "            # For continuous data (MSE loss), the MADE output is treated as the predicted mean.\n",
        "            # We use this predicted mean as the sample (since we trained with MSE).\n",
        "            predicted_value = output[0, i]\n",
        "\n",
        "            # Set the predicted value back into the input vector for the next step.\n",
        "            x_made[i] = predicted_value\n",
        "\n",
        "    # 3. Extract and Reshape the Generated X_post\n",
        "    # The generated post-disaster vector is the second half of x_made\n",
        "    generated_post_flat = x_made[start_index:]\n",
        "\n",
        "    # Reshape: (C, H, W) -> (3, 32, 32)\n",
        "    generated_post_patch = generated_post_flat.reshape(image_bands, patch_size, patch_size)\n",
        "\n",
        "    return generated_post_patch\n",
        "\n",
        "def visualize_samples(pre_patch_tensor, generated_post_tensor, original_post_tensor=None):\n",
        "    \"\"\"Visualizes the pre-disaster, generated, and optionally the original post-disaster patches.\"\"\"\n",
        "\n",
        "    # Ensure tensors are on CPU and convert to image format (C, H, W)\n",
        "    pre_patch = pre_patch_tensor.cpu()\n",
        "    generated_post = generated_post_tensor.cpu().clamp(0, 1) # Clamp to [0, 1] range\n",
        "\n",
        "    images = [pre_patch, generated_post]\n",
        "    titles = [\"Pre-Disaster (Input)\", \"Generated Post-Disaster\"]\n",
        "\n",
        "    if original_post_tensor is not None:\n",
        "        images.append(original_post_tensor.cpu())\n",
        "        titles.append(\"Original Post-Disaster (Ground Truth)\")\n",
        "\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(5 * len(images), 5))\n",
        "\n",
        "    if len(images) == 1:\n",
        "        axes = [axes] # Ensure axes is iterable for a single image case\n",
        "\n",
        "    for ax, img, title in zip(axes, images, titles):\n",
        "        # Convert tensor (C, H, W) to image format (H, W, C)\n",
        "        img_np = img.permute(1, 2, 0).numpy()\n",
        "        ax.imshow(img_np)\n",
        "        ax.set_title(title, fontsize=14)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LNjhWl84qr1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Load Data ---\n",
        "# Instantiate the full dataset (uses the XView2MADETransformDataset defined earlier)\n",
        "full_dataset = XView2MADETransformDataset(image_dir=IMAGES_PATH, patch_size=PATCH_SIZE)\n",
        "total_samples = len(full_dataset)\n",
        "\n",
        "# Split sizes from your training run [cite: 308-310]\n",
        "train_size = int(0.8 * total_samples)\n",
        "val_size = total_samples - train_size\n",
        "\n",
        "# NOTE: The training split used here must match the split used for training the model.\n",
        "# Ensure 'random_split' is imported (already handled in section 1).\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "print(f\"Total Validation Samples: {len(val_dataset)}\")\n",
        "\n",
        "# --- 2. Instantiate and Load Model ---\n",
        "made_model = MADE(\n",
        "    input_dim=INPUT_DIM,\n",
        "    hidden_size=HIDDEN_SIZE,\n",
        "    num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
        "    output_dim=OUTPUT_DIM\n",
        ").to(device)\n",
        "\n",
        "# --- 3. Run Inference on a Sample ---\n",
        "\n",
        "# Get a sample from the validation set (e.g., the 5th sample)\n",
        "test_sample_idx = 4\n",
        "test_sample_vector = val_dataset[test_sample_idx]\n",
        "\n",
        "# --- Extract Pre and Post Patches (for Ground Truth comparison) ---\n",
        "# The MADE vector is [Flattened Pre-Patch, Flattened Post-Patch]\n",
        "pixel_count = PATCH_SIZE * PATCH_SIZE * IMAGE_BANDS\n",
        "pre_flat_ground_truth = test_sample_vector[:pixel_count]\n",
        "post_flat_ground_truth = test_sample_vector[pixel_count:]\n",
        "\n",
        "# Reshape to (C, H, W)\n",
        "pre_patch_tensor = pre_flat_ground_truth.reshape(IMAGE_BANDS, PATCH_SIZE, PATCH_SIZE)\n",
        "original_post_tensor = post_flat_ground_truth.reshape(IMAGE_BANDS, PATCH_SIZE, PATCH_SIZE)\n",
        "\n",
        "\n",
        "# --- GENERATE THE POST-DISASTER IMAGE ---\n",
        "print(\"\\nStarting Autoregressive Sampling...\")\n",
        "\n",
        "# The MADE model is now used to sample the post-disaster image\n",
        "generated_post_patch = made_sample_post_disaster(\n",
        "    model=made_model,\n",
        "    pre_patch_tensor=pre_patch_tensor,\n",
        "    patch_size=PATCH_SIZE,\n",
        "    image_bands=IMAGE_BANDS,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# --- 4. Visualize Results ---\n",
        "visualize_samples(pre_patch_tensor, generated_post_patch, original_post_tensor)\n",
        "print(\"\\nGenerated image displayed above. The goal is for the 'Generated Post-Disaster' image to closely resemble the 'Original Post-Disaster' image.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "id": "4dINSNUnqtg0",
        "outputId": "43526e72-1036-43b1-dc15-c186b6e8ae1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Indexing Image Patches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2799/2799 [01:35<00:00, 29.41it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Validation Samples: 560\n",
            "\n",
            "Starting Autoregressive Sampling...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated image displayed above. The goal is for the 'Generated Post-Disaster' image to closely resemble the 'Original Post-Disaster' image.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fOk1_pMbr85fVhC2EuzBNlaNsAzrwRBl",
      "authorship_tag": "ABX9TyMRr8aEumAvkVlMRBUqxMjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}